{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJf0YRjuPd3k/XdDUF+cyZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DebicharanTripathy/DebicharanTripathy/blob/main/m22ai545_Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "nVUVagF21ebB",
        "outputId": "ed177c7d-25f1-44b8-e564-ee19ec47d7ab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5ffb1bac8fb4>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    import tensorflow as tf from sklearn.metrices\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Author: Debicharan Tripathy\n",
        "# Roll no: m22ai545\n",
        "\n",
        "# Task 2: as per GPT Download the data from http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz\n",
        "# Execute it by updating the path.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf;\n",
        "import tensorflow as tf from sklearn.metrices\n",
        "import confusion_matrix, roc_auc_score from sklearn.model_selection\n",
        "import train_test_split from tensorflow.keras\n",
        "import layers, models, optimizers from tensorflow.keras.preprocessing.image\n",
        "import ImageDataGenerator from tensorflow.keras.applications\n",
        "import VGG16\n",
        "\n",
        "# Task 2.1: Preprocess of the STL-10 dataset\n",
        "data_dir = 'stl10_data/'\n",
        "train_images = np.load(os.path.join(data_dir, 'train_images.npy'))\n",
        "train_labels = np.load(os.path.join(data_dir, 'train_labels.npy'))\n",
        "test_images = np.load(os.path.join(data_dir, 'test_images.npy'))\n",
        "test_labels = np.load(os.path.join(data_dir, 'test_labels.npy'))\n",
        "\n",
        "# Preprocess of the images (e.g., resizing, normalization)\n",
        "\n",
        "# Divide the dataset into sets for training and validation.\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Task 2.2: Use the encoder of a trained autoencoder as a feature extractor.\n",
        "autoencoder = models.load_model('pretrained_autoencoder.h5')\n",
        "encoder = autoencoder.layers[1]  # assuming that the second layer is the encoder\n",
        "\n",
        "# Extract features from the images\n",
        "train_features = encoder.predict(train_images)\n",
        "val_features = encoder.predict(val_images)\n",
        "test_features = encoder.predict(test_images)\n",
        "\n",
        "# Task 2.3: Build MLP classifiers\n",
        "hidden_layers_3 = [256, 128, 64]\n",
        "hidden_layers_5 = [512, 256, 128, 64, 32]\n",
        "\n",
        "def build_mlp_classifier(hidden_layers):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Flatten(input_shape=train_features.shape[1:]))\n",
        "    for units in hidden_layers:\n",
        "        model.add(layers.Dense(units, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# Create an MLP classifier using three hidden layers.\n",
        "classifier_3 = build_mlp_classifier(hidden_layers_3)\n",
        "classifier_3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "classifier_3.fit(train_features, train_labels, validation_data=(val_features, val_labels), epochs=10)\n",
        "\n",
        "# Create an MLP classifier using five hidden layers.\n",
        "classifier_5 = build_mlp_classifier(hidden_layers_5)\n",
        "classifier_5.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "classifier_5.fit(train_features, train_labels, validation_data=(val_features, val_labels), epochs=10)\n",
        "\n",
        "# Task 2.4: Fine-tune classifiers with the different training sample percentages\n",
        "train_samples = [0.01, 0.1, 0.2, 0.4, 0.6]\n",
        "classifiers = [classifier_3, classifier_5]\n",
        "results = {}\n",
        "\n",
        "for classifier in classifiers:\n",
        "    classifier_results = []\n",
        "    for sample_percent in train_samples:\n",
        "        # Take a sample at random from the training data.\n",
        "        sample_indices = np.random.choice(len(train_features), int(sample_percent * len(train_features)), replace=False)\n",
        "        sampled_features = train_features[sample_indices]\n",
        "        sampled_labels = train_labels[sample_indices]\n",
        "\n",
        "        # Classifier refinement using sampling data\n",
        "        classifier.fit(sampled_features, sampled_labels, epochs=5, verbose=0)\n",
        "\n",
        "        # Evaluate the classifier on test set\n",
        "        test_loss, test_acc = classifier.evaluate(test_features, test_labels, verbose=0)\n",
        "\n",
        "        classifier_results.append(test_acc)\n",
        "\n",
        "    results[classifier.name] = classifier_results\n",
        "\n",
        "# Task 2.5: Evaluate the performance using confusion matrix and AUC-ROC curve\n",
        "def evaluate_performance(classifier, features, labels):\n",
        "    predictions = classifier.predict(features)\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "    cm = confusion_matrix(labels, y_pred)\n",
        "    auc_roc = roc_auc_score(labels, predictions, multi_class='ovr')\n",
        "    return cm, auc_roc\n",
        "\n",
        "# Evaluate the performance for Task 2.3.a and 2.3.b\n",
        "cm_3, auc_roc_3 = evaluate_performance(classifier_3, test_features, test_labels)\n",
        "cm_5, auc_roc_5 = evaluate_performance(classifier_5, test_features, test_labels)\n",
        "\n",
        "# Evaluate the performance for Task 2.4.a-e\n",
        "cm_results = {}\n",
        "auc_roc_results = {}\n",
        "\n",
        "for classifier_name, classifier_results in results.items():\n",
        "    cm, auc_roc = evaluate_performance(classifiers[classifier_name], test_features, test_labels)\n",
        "    cm_results[classifier_name] = cm\n",
        "    auc_roc_results[classifier_name] = auc_roc\n",
        "\n",
        "# Task 2.6: Implement a different architecture for improvement (e.g., CNN)\n",
        "def build_cnn_classifier():\n",
        "    base_model = VGG16(include_top=False, weights='imagenet', input_shape=train_images.shape[1:])\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "    model = models.Sequential()\n",
        "    model.add(base_model)\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "    model.add(layers.Dense(256, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "cnn_classifier = build_cnn_classifier()\n",
        "cnn_classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "cnn_classifier.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=10)\n",
        "\n",
        "# Analyse the performance of the CNN classifier.\n",
        "cm_cnn, auc_roc_cnn = evaluate_performance(cnn_classifier, test_images, test_labels)"
      ]
    }
  ]
}